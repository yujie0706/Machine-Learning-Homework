{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "07be1928242c42efc713357730139864",
     "grade": false,
     "grade_id": "cell-5ae594183c2ba9fb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Homework 3: Logistic Regression and Stochastic Gradient Descent\n",
    "\n",
    "\n",
    "\n",
    "This assignment is due on Canvas by **11:59pm on Sunday April 23**. \n",
    "Your solutions to theoretical questions should be done in Markdown/MathJax directly below the associated question.\n",
    "Your solutions to computational questions should include any specified Python code and results \n",
    "as well as written commentary on your conclusions.\n",
    "Remember that you are encouraged to discuss the problems with your instructors and classmates, \n",
    "but **you must write all code and solutions on your own**. For a refresher on the course **Collaboration Policy** click [here](https://canvas.uchicago.edu/courses/49007).\n",
    "\n",
    "**NOTES**: \n",
    "\n",
    "- Some problems with code may be autograded.  If we provide a function API, **do not** change it.  If we do not provide a function API, then you're free to structure your code however you like. \n",
    "- Submit this Jupyter notebook to Canvas.  Do not compress it using tar, rar, zip, etc.\n",
    "- Also submit a PDF of this Jupyter notebook to Canvas.\n",
    "\n",
    "**Acknowledgment**: Noah Smith, Chris Ketelsen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fc73d2e329ca249a17292d258bb2f254",
     "grade": false,
     "grade_id": "cell-831d67dea055c2cb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c13211f1e8a0a6ccbebb6fe42bcde5b4",
     "grade": false,
     "grade_id": "cell-96a1ce6a1ab8dabc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## [100 points] Logistic Regression + SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c693a75f278f438118a14fa7aebc5f57",
     "grade": false,
     "grade_id": "cell-051a2362799a8530",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this assignment, you'll implement a Logistic Regression classifier to predict whether a reported crime (incident) results in an arrest.  We use the Chicago crime dataset used in Assignment 1, with a few modifications described below.  \n",
    "\n",
    "\n",
    "Dataset has following attributes:\n",
    "\n",
    "|Variable|Definition|Key|\n",
    "|:----:|:----:|:---|\n",
    "|Hour|Time of incident|integer|\n",
    "|Domestic|Category of Crime|bool|\n",
    "|Primary Type |Type of Crime|one hot encoded (all types with moderate arrest rate)|\n",
    "|Ward\t|Location of incident|one hot encoded|\n",
    "|Arrest\t|Whether an arrest was made|bool|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "773d9d1a5ff41f0be0c11de340a54b4e",
     "grade": false,
     "grade_id": "cell-81b4a35e847dde6f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The following cell is a class to load the crime dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \"\"\"\n",
    "    Class to load dataset containing Chicago crime features\n",
    "    You shouldn't have to modify this class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, location, random_state=1241):\n",
    "        # Load the dataset\n",
    "        np.random.seed(random_state)\n",
    "        small_df = pd.read_csv(location)\n",
    "        y_crime_df, X_crime_df  = small_df[['Arrest']], small_df.drop(['Arrest'], axis=1)\n",
    "        self.train_x, self.test_x, self.train_y, self.test_y = train_test_split(\n",
    "            X_crime_df.to_numpy(), y_crime_df.to_numpy(), test_size=0.2, random_state=123)\n",
    "        \n",
    "        # appending biases\n",
    "        self.train_x = np.concatenate((np.ones((self.train_x.shape[0], 1)), self.train_x), axis=1)\n",
    "        self.test_x = np.concatenate((np.ones((self.test_x.shape[0], 1)), self.test_x), axis=1)\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def shuffle(X, y):\n",
    "        \"\"\" Shuffle training data \"\"\"\n",
    "        shuffled_indices = np.random.permutation(len(y))\n",
    "        return X[shuffled_indices], y[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4d8b8360adcdc245a30b0246a1c3e738",
     "grade": false,
     "grade_id": "cell-29b47ae8fec3862a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Part 1 [10 points]: Implementing sigmoid\n",
    "\n",
    "#### Part 1 A [7 points] \n",
    "First, implement the `sigmoid` function to return the output by applying the sigmoid function $\\sigma(z)$ to the input parameter, where the sigmoid function $\\sigma(z)$ is defined as:\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1+e^{-z}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cc614d4d4c6ae3c0a92231061375a903",
     "grade": false,
     "grade_id": "cell-389deffcd0e8b595",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(score, threshold=20.0):\n",
    "    \"\"\"\n",
    "    Sigmoid function with a threshold\n",
    "    :param score: (float) A real valued number to convert into a number between 0 and 1 \n",
    "    :param threshold: (float) Prevent overflow of exp by capping activation at 20. \n",
    "                    (e.g., scores higher than 20 are converted to 20, scores lower than -20 are converted to -20).                 \n",
    "    \n",
    "    :return: (float) sigmoid function result.\n",
    "    \"\"\"\n",
    "    if score < -threshold:\n",
    "        score = -threshold\n",
    "    elif score > threshold:\n",
    "        score = threshold\n",
    "    sigm = 1 / (1 + math.exp(-score))\n",
    "    return sigm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4594a8fbce21c73aa41f8a76bed75a37",
     "grade": true,
     "grade_id": "cell-73fb14b862081aa9",
     "locked": true,
     "points": 7,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999979388463\n",
      "0.9999999979388463\n"
     ]
    }
   ],
   "source": [
    "# verify sigmoid implemention w/ scipy;\n",
    "# note: you should NOT use scipy for your implementation!\n",
    "from scipy.stats import logistic\n",
    "assert sigmoid(1) == logistic.cdf(1)\n",
    "assert sigmoid(5) == logistic.cdf(5)\n",
    "assert sigmoid(100, threshold=20) == logistic.cdf(20)\n",
    "assert sigmoid(-1) == logistic.cdf(-1)\n",
    "assert sigmoid(-5) == logistic.cdf(-5)\n",
    "assert sigmoid(-100, threshold=20) == logistic.cdf(-20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e45625adb13b0e423e9b1143826cb3b3",
     "grade": false,
     "grade_id": "cell-cf2ae3f924808a05",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Part 1 B [3 points]\n",
    "\n",
    "Next, implement the derivative of the `sigmoid` function, `sigmoid_grad`, i.e. $\\frac{\\partial\\sigma(x)}{\\partial x}$.\n",
    "\n",
    "Hint: your implementation of `sigmoid_grad` should be able to use  your `sigmoid` function to compute the derivative!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cfdafae2fdb03837a067e6500ee33501",
     "grade": false,
     "grade_id": "cell-71a4ae76610d957e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid_grad(y, threshold=20.0):\n",
    "    \"\"\"\n",
    "    Derivative/gradient of the sigmoid function.\n",
    "    :param y: (float) A real valued input for which to compute the derivative.\n",
    "    :param threshold: (float) Prevent overflow of exp by capping activation at 20.\n",
    "    \n",
    "    :return: (float) sigmoid derivative function result.\n",
    "    \"\"\"\n",
    "    grad = sigmoid(y, threshold) * (1 - sigmoid(y, threshold))\n",
    "    return grad\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b3095babb98e62583a4f0822b571661b",
     "grade": true,
     "grade_id": "cell-cac94bfa1729f2b9",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# verify sigmoid_grad using numerical differentiation, i.e: f(x+h)-f(x-h) / 2h\n",
    "epsilon = 1.0E-8\n",
    "assert np.isclose(sigmoid_grad(1.0), (sigmoid(1.0 + epsilon) - sigmoid(1.0 - epsilon)) / (2.0*epsilon))\n",
    "assert np.isclose(sigmoid_grad(0.1), (sigmoid(0.1 + epsilon) - sigmoid(0.1 - epsilon)) / (2.0*epsilon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "99b0b3cb05e58cabccbeacc673c44c0b",
     "grade": false,
     "grade_id": "cell-8cff871369e65e2d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Part 2 [80 points]\n",
    "\n",
    "#### Part 2 A [20 points]\n",
    "\n",
    "The negative log likelihood objective is defined as:\n",
    "$$\n",
    "\\textrm{NLL}(\\boldsymbol{\\beta}) = -\\displaystyle\\sum_{i=1}^n \\left[y_i \\log \\sigma(\\boldsymbol{\\beta}^T{\\bf x}^{(i)}) + (1-y_i)\\log(1 - \\sigma(\\boldsymbol{\\beta}^T{\\bf x}^{(i)}))\\right] \n",
    "$$\n",
    "\n",
    "First, write down the derivative of the negative log likelihood objective function, with respect to $\\boldsymbol{\\beta}$. Since we are working with SGD, derive it for  $n=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c967c69698e10c8706bfd3e24dc6cfd4",
     "grade": true,
     "grade_id": "cell-18bc846ce0e4d6b3",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3333819f9a33c14f73c4dd51200e03ad",
     "grade": false,
     "grade_id": "cell-747488c656351f6e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next, using the `sigmoid` function implemented earlier, finish the `sgd_update` function so that it performs stochastic gradient descent on the single training example and updates the weight vector correspondingly without regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b84ed8ba2ee35a3afe25b75684a4ed5e",
     "grade": false,
     "grade_id": "cell-26bd636741b8b10c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class LogReg:\n",
    "    def __init__(self, num_features, eta):\n",
    "        \"\"\"\n",
    "        Create a logistic regression classifier\n",
    "        :param num_features: (int) The number of features (including bias)\n",
    "        :param eta: (float) learning rate\n",
    "        \"\"\"\n",
    "        self.w = np.zeros(num_features)\n",
    "        self.eta = eta\n",
    "\n",
    "    def progress(self, examples_x, examples_y):\n",
    "        \"\"\"\n",
    "        Given a set of examples, compute the probability and accuracy\n",
    "        :param examples_x: (2D np.ndarray) The features from the dataset to score\n",
    "        :param examples_y: (1D np.ndarray) The labels from the dataset to score\n",
    "\n",
    "        :return: (float, float) A tuple of (log probability, accuracy)\n",
    "        \"\"\"\n",
    "\n",
    "        logprob = 0.0\n",
    "        num_right = 0\n",
    "        for x_i, y in zip(examples_x, examples_y):\n",
    "            p = sigmoid(self.w.dot(x_i))\n",
    "            if y == 1:\n",
    "                logprob += math.log(p)\n",
    "            else:\n",
    "                logprob += math.log(1.0 - p)\n",
    "\n",
    "            # Get accuracy\n",
    "            if abs(y - p) <= 0.5:\n",
    "                num_right += 1\n",
    "\n",
    "        return logprob, float(num_right) / float(len(examples_y))\n",
    "\n",
    "    def sgd_update(self, x_i, y, lam=0.0):\n",
    "        \"\"\"\n",
    "        Compute a stochastic gradient update to improve the log likelihood.\n",
    "        :param x_i: (1D np.ndarray) The features of the example to take the gradient with respect to\n",
    "        :param y: (float) The target output of the example to take the gradient with respect to\n",
    "        :param lam: (float) regularization term. Default is zero; only used in Part 2D.\n",
    "        \n",
    "        :return: (1D np.ndarray) Return the new value of the regression coefficients\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Finish this function to do a single stochastic gradient descent update\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return self.w\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "24b46275cea3a8badc19a1e7411a430c",
     "grade": true,
     "grade_id": "cell-028c202a2089443e",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testPosUnregUpdate (tests.tests.TestLogReg)\n",
      "test update based on positive example ... ERROR\n",
      "testNegUnregUpdate (tests.tests.TestLogReg)\n",
      "test update based on negative example ... ERROR\n",
      "\n",
      "======================================================================\n",
      "ERROR: testPosUnregUpdate (tests.tests.TestLogReg)\n",
      "test update based on positive example\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yujie0706/Machine-Learning-Homework/hw3/tests/tests.py\", line 26, in testPosUnregUpdate\n",
      "    lr.sgd_update(train_x[i],train_y[i], lam=0)\n",
      "  File \"/tmp/user/23071/ipykernel_1596183/2969371505.py\", line 48, in sgd_update\n",
      "    raise NotImplementedError()\n",
      "NotImplementedError\n",
      "\n",
      "======================================================================\n",
      "ERROR: testNegUnregUpdate (tests.tests.TestLogReg)\n",
      "test update based on negative example\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yujie0706/Machine-Learning-Homework/hw3/tests/tests.py\", line 41, in testNegUnregUpdate\n",
      "    lr.sgd_update(train_x[i],train_y[i], lam=0)\n",
      "  File \"/tmp/user/23071/ipykernel_1596183/2969371505.py\", line 48, in sgd_update\n",
      "    raise NotImplementedError()\n",
      "NotImplementedError\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.004s\n",
      "\n",
      "FAILED (errors=2)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "one or more tests for prob 2A failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtests\u001b[39;00m \u001b[39mimport\u001b[39;00m tests\n\u001b[0;32m----> 2\u001b[0m tests\u001b[39m.\u001b[39;49mrun_test_suite(\u001b[39m'\u001b[39;49m\u001b[39mprob 2A\u001b[39;49m\u001b[39m'\u001b[39;49m, LogReg)\n",
      "File \u001b[0;32m~/Machine-Learning-Homework/hw3/tests/tests.py:81\u001b[0m, in \u001b[0;36mrun_test_suite\u001b[0;34m(name, ctor)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[39mfor\u001b[39;00m test \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mtestPosUnregUpdate\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mtestNegUnregUpdate\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m     80\u001b[0m         prob2A\u001b[39m.\u001b[39maddTest(TestLogReg(test, ctor))\n\u001b[0;32m---> 81\u001b[0m     \u001b[39massert\u001b[39;00m unittest\u001b[39m.\u001b[39mTextTestRunner(verbosity\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mrun(prob2A)\u001b[39m.\u001b[39mwasSuccessful(), \u001b[39m\"\u001b[39m\u001b[39mone or more tests for prob 2A failed\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     82\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mprob 2E\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     83\u001b[0m     prob2A \u001b[39m=\u001b[39m unittest\u001b[39m.\u001b[39mTestSuite()\n",
      "\u001b[0;31mAssertionError\u001b[0m: one or more tests for prob 2A failed"
     ]
    }
   ],
   "source": [
    "from tests import tests\n",
    "tests.run_test_suite('prob 2A', LogReg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "25fdcb8fb12a3d18ffcd4e8843d1b655",
     "grade": false,
     "grade_id": "cell-736f116c1e26304b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Part 2 B [20 points]\n",
    "Complete the code below to loop over the training data and perform stochastic gradient descent for a pre-defined number of epochs. You do not need to use the parameters lam and decay for this part.\n",
    "\n",
    "Note: remember to shuffle your training data using `Dataset.shuffle` at the beginning of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "841f16438b76a2166efbb104e6cf2c51",
     "grade": false,
     "grade_id": "cell-a90025845b0cca0f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train(epochs, eta, store_epoch, lam=0, decay=0):\n",
    "    \"\"\"\n",
    "    Train a LogReg object for a set number of epochs with a given eta.\n",
    "    \n",
    "    :param epochs: (int) total number of training epochs\n",
    "    :param eta: (float) learning rate\n",
    "    :param store_epoch: (int) store training and test accuracies every store_epoch epochs\n",
    "    :param lam: (float) weight given to regularization term. Default 0. Only used in Part 2D. \n",
    "    :param decay: (float) Used to update learning rate during training (Part 3). \n",
    "                  Equals 0 when learning rate is constant throughout training (Part 2). \n",
    "                  \n",
    "    :return (train_accuracy_array, test_accuracy_array, learning_rates): tuple of (List, List, List)\n",
    "        :train_accuracy_array: training accuracy after every store_epoch epochs\n",
    "        :test_accuracy_array: test accuracy after every store_epoch epochs\n",
    "        :learning_rates: learning rate after every store_epoch epochs. All values in this list \n",
    "                         will be the same if decay = 0 (Only required for Part 2F)\n",
    "    \n",
    "        Example: With epochs = 30 and store_epoch = 10, only store accuracies after epochs = 10, 20, and 30.\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset_handler = Dataset('data/crime.csv')\n",
    "    lr = LogReg(dataset_handler.train_x.shape[1], eta)\n",
    "\n",
    "    assert dataset_handler.train_x.shape == (1105, 60) \n",
    "    assert dataset_handler.test_x.shape == (277, 60) \n",
    "    \n",
    "    train_accuracy_array = []\n",
    "    test_accuracy_array = []\n",
    "    learning_rates = []\n",
    "    for epoch in range(epochs):\n",
    "        # TODO: Finish the code to loop over the training data and perform a stochastic\n",
    "        # gradient descent update on each training example.\n",
    "\n",
    "        # NOTE: It may be helpful to call upon the 'progress' method in the LogReg class\n",
    "        # to make sure the algorithm is truly learning properly on both training and test data\n",
    "        \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return train_accuracy_array, test_accuracy_array, learning_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f47c3a6fb3120b72912aaa4039c087e2",
     "grade": true,
     "grade_id": "cell-c8ab6db3236a4ff2",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "eta  = 1e-4\n",
    "epochs = 300\n",
    "store_epoch = 50\n",
    "train_acc, test_acc, _ = train(epochs, eta, store_epoch)\n",
    "\n",
    "for i in range(len(train_acc)):\n",
    "    print(\"\\ntrain accuracy after {} epochs: {}\".format((i+1)*store_epoch, train_acc[i]))\n",
    "    print(\"test accuracy after {} epochs: {}\".format((i+1)*store_epoch, test_acc[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a4f5d01581748dc0927642c05d468e05",
     "grade": false,
     "grade_id": "cell-c7150ec75f715a87",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Part 2 C [10 points]\n",
    "What is the role of the learning rate? What are the pros and cons of high/low learning rates? Do you see any trade-off? First, plot accuracies of different $\\eta$s together vs. number of epochs for both training and testing. Then briefly elaborate on these questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "31c388ef355654ef9637244d20dd99bd",
     "grade": true,
     "grade_id": "cell-09a16c6562efd6e5",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_handler = Dataset('data/crime.csv')\n",
    "train_results = {}\n",
    "test_results = {}\n",
    "\n",
    "epochs = 400\n",
    "store_epoch = 10\n",
    "for eta in [1e-3, 1e-4, 1e-5, 1e-6]:\n",
    "\n",
    "    # TODO: \n",
    "    # Finish the code to loop over different values of learning rates (Use the train() function above)\n",
    "    \n",
    "    # You need to store accuracy arrays obtained in the dictionaries provided \n",
    "    # above (train_results and test_results)\n",
    "    \n",
    "    # Effectively, you will be creating a mapping between eta -> train/test_accuracy_array \n",
    "    # Therefore, running train_results[eta] should return the train_accuracy_array for that value\n",
    "    # of eta and likewise for test_results[eta].\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "19e321915541406dc1e5e2b8b6ffd90c",
     "grade": false,
     "grade_id": "cell-cdceadb514539e82",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Plot training results below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12,6))\n",
    "epochs_array = [i for i in range(store_epoch, epochs, store_epoch)]\n",
    "epochs_array.append(epochs)\n",
    "ax.plot(epochs_array, train_results[1e-3], color=\"steelblue\", label='1e-03')\n",
    "ax.plot(epochs_array, train_results[1e-4], color=\"lightblue\", label='1e-04')\n",
    "ax.plot(epochs_array, train_results[1e-5], color=\"grey\", label='1e-05')\n",
    "ax.plot(epochs_array, train_results[1e-6], color=\"black\", label='1e-06')\n",
    "ax.legend(loc=\"lower right\", fontsize=16)\n",
    "ax.set_xlabel(\"epochs\", fontsize=16)\n",
    "ax.set_ylabel(\"train accuracy\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3d69bdf93baa5b77689c4c724361c7e4",
     "grade": false,
     "grade_id": "cell-08d14ca96a171d0c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Plot testing results below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12,6))\n",
    "epochs_array = [i for i in range(store_epoch,epochs,store_epoch)]\n",
    "epochs_array.append(epochs)\n",
    "ax.plot(epochs_array, test_results[1e-3], color=\"steelblue\", label='1e-03')\n",
    "ax.plot(epochs_array, test_results[1e-4], color=\"lightblue\", label='1e-04')\n",
    "ax.plot(epochs_array, test_results[1e-5], color=\"grey\", label='1e-05')\n",
    "ax.plot(epochs_array, test_results[1e-6], color=\"black\", label='1e-06')\n",
    "ax.legend(loc=\"lower right\", fontsize=16)\n",
    "ax.set_xlabel(\"epochs\", fontsize=16)\n",
    "ax.set_ylabel(\"test accuracy\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a4b1d9f962a2ddedfdf79ec74c657b3c",
     "grade": true,
     "grade_id": "cell-dd0ea201d734cc98",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cdd7290a7901f8acddddf59facc380b4",
     "grade": false,
     "grade_id": "cell-edbacfc3a57559ab",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "#### Part 2 D [15 points]\n",
    "\n",
    "Adding $l_2$ regularization to the feature parameters for NLL loss gives:\n",
    "\n",
    "$$\n",
    "\\textrm{NLL}_{l_2}(\\boldsymbol{\\beta}) = -\\displaystyle\\sum_{i=1}^n \\left[y_i \\log \\sigma(\\boldsymbol{\\beta}^T{\\bf x}^{(i)}) + (1-y_i)\\log(1 - \\sigma(\\boldsymbol{\\beta}^T{\\bf x}^{(i)}))\\right] + \\lambda\\displaystyle\\sum_{k=1}^{p} \\beta_{k}^2\n",
    "$$\n",
    "\n",
    "where $p$ is the number of features, and $\\beta_0$ is the bias term. Notice that $\\beta_0$ is not included in the regularization term.\n",
    "\n",
    "Write down the derivative of the regularized negative log likelihood loss function $\\textrm{NLL}_{l_2}$ with respect to $\\boldsymbol{\\beta}$. Since we are working with SGD, derive it for $n=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d2a3be5d22da40d3f31be6108add2398",
     "grade": true,
     "grade_id": "cell-a25a04527ff025ff",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "91255fcddea6e4116092f3b764264982",
     "grade": false,
     "grade_id": "cell-58a1174637893818",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Update your implementation of the `sgd_update` method so that it performs regularized SGD updates of the model parameters to minimize the regularized NLL loss function.\n",
    "\n",
    "Remember, do **not** regularize the bias parameter $\\beta_0$.\n",
    "\n",
    "Provide train and test accuracy after above change with `lam=1e-5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d1038096122c38c0d8b78e6c4b293383",
     "grade": true,
     "grade_id": "cell-3879c5176d481064",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tests import tests\n",
    "tests.run_test_suite('prob 2E', LogReg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed3b4fe563783325ebfe27275f8a9fd5",
     "grade": false,
     "grade_id": "cell-8598434febfab063",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Part 2 E [8 points]\n",
    "Update your implementation of train() to incorporate a regularization term. The change should typically be on only one line in your code.\n",
    "\n",
    "Plot accuracies of different $\\lambda$s together vs. epochs for both training and testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cbbd2371db7186978670f8f447a5dc17",
     "grade": true,
     "grade_id": "cell-a450215e16c0f1a5",
     "locked": false,
     "points": 8,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dataset_handler = Dataset('data/crime.csv')\n",
    "train_results = {}\n",
    "test_results = {}\n",
    "epochs = 400\n",
    "store_epoch = 10\n",
    "eta = 1e-4\n",
    "for lam in [0, 0.1, 0.05, 0.01]:\n",
    "    \n",
    "    # TODO: \n",
    "    # Finish the code to loop over different values of lambda (Use the train() function above)\n",
    "    \n",
    "    # You need to store accuracy arrays obtained in the dictionaries provided \n",
    "    # above (train_results and test_results)\n",
    "    \n",
    "    # Effectively, you will be creating a mapping between lambda -> train/test_accuracy_array \n",
    "    # Therefore, running train_results[lam] should return the train_accuracy_array for that value\n",
    "    # of lam and likewise for test_results[lam].\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "117526c11d85393af4d2de0c81c3b33e",
     "grade": false,
     "grade_id": "cell-48bd45286d3e1d41",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Plot training results below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12,6))\n",
    "epochs_array = [i for i in range(store_epoch,epochs,store_epoch)]\n",
    "epochs_array.append(epochs)\n",
    "ax.plot(epochs_array, train_results[0], color=\"black\", label=str(0))\n",
    "ax.plot(epochs_array, train_results[0.01], color=\"grey\", label=str(0.01))\n",
    "ax.plot(epochs_array, train_results[0.05], color=\"lightblue\", label=str(0.05))\n",
    "ax.plot(epochs_array, train_results[0.1], color=\"steelblue\", label=str(0.1))\n",
    "ax.legend(loc=\"lower right\", fontsize=16)\n",
    "ax.set_xlabel(\"epochs\", fontsize=16)\n",
    "ax.set_ylabel(\"train accuracy\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e518dd444497901ecdfcd4e4907a6f25",
     "grade": false,
     "grade_id": "cell-dbc673e5af14643b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Plot testing results below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12,6))\n",
    "epochs_array = [i for i in range(store_epoch,epochs,store_epoch)]\n",
    "epochs_array.append(epochs)\n",
    "ax.plot(epochs_array, test_results[0], color=\"black\", label=str(0))\n",
    "ax.plot(epochs_array, test_results[0.01], color=\"grey\", label=str(0.01))\n",
    "ax.plot(epochs_array, test_results[0.05], color=\"lightblue\", label=str(0.05))\n",
    "ax.plot(epochs_array, test_results[0.1], color=\"steelblue\", label=str(0.1))\n",
    "ax.legend(loc=\"lower right\", fontsize=16)\n",
    "ax.set_xlabel(\"epochs\", fontsize=16)\n",
    "ax.set_ylabel(\"test accuracy\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4db9f6464976435e7659d8a41d35f6b3",
     "grade": false,
     "grade_id": "cell-e3a64883a284043e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Part 2 F [7 points] \n",
    "What is the effect of regularization term with respect to accuracy? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "699b1723a6fd7d71789454c6e6e1c715",
     "grade": true,
     "grade_id": "cell-0ce393d045320da5",
     "locked": false,
     "points": 7,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b5e12d4b12169ae4902ec1a611cd054d",
     "grade": false,
     "grade_id": "cell-58303538bf6d7940",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Part 3 [10 points] \n",
    "\n",
    "Time based Learning Rate is dynamic learning rate given the following equation:\n",
    "\n",
    "$\\textrm{LearningRate} = \\eta\\, / \\,(1 + \\textrm{decay} \\cdot \\textrm{current epoch})$\n",
    "\n",
    "Train SGD with the dynamic learning rate defined above and follow these instructions:\n",
    "* Use initial learning rate $\\eta = 0.1$.\n",
    "* Use $\\textrm{decay} = 0.001$.\n",
    "* Update learning rate `lr.eta` every epoch.\n",
    "* Use $\\lambda = 0$ (no regularization)\n",
    "* Plot train accuracy and learning rate together for each epoch.\n",
    "\n",
    "The above can be accomplished by changing one line in `train()` from Part 2a. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aac67f252023aaaed42899e74fa01981",
     "grade": true,
     "grade_id": "cell-4cc9c3502840b913",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "eta  = 1e-1\n",
    "epochs = 200\n",
    "store_epoch = 1\n",
    "\n",
    "# Lists required for plotting\n",
    "train_accuracy_array = None\n",
    "learning_rates = None\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12,6))\n",
    "epochs_array = [i for i in range(1,epochs)]\n",
    "epochs_array.append(epochs)\n",
    "ax.plot(epochs_array, train_accuracy_array, color=\"steelblue\", label=str('train accuracy'))\n",
    "ax.plot(epochs_array, learning_rates,color=\"grey\", label=str('learning rate'))\n",
    "ax.legend(loc=\"center right\", fontsize=16)\n",
    "ax.set_xlabel(\"epochs\", fontsize=16)\n",
    "ax.set_ylabel(\"\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional survey.\n",
    "***\n",
    "\n",
    "We are always interested in your feedback. At the end of each homework, there is a simple anonymous feedback [survey](https://docs.google.com/forms/d/e/1FAIpQLSefzoYvcweZYAb_OsNnEyMfbdyyTaVFcOJAnEanwrYSGtNpIQ/viewform) to solicit your feedback for how to improve the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "396.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
