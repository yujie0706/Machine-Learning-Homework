{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "962fa5e92993ce9aafbf193d2ea11109",
     "grade": false,
     "grade_id": "intro",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Homework 1: Decision tree and K-nearest neighbor\n",
    "\n",
    "\n",
    "\n",
    "This assignment is due on Canvas by **11:59pm on Friday, March 31**. \n",
    "Your solutions to theoretical questions should be done in Markdown/MathJax directly below the associated question.\n",
    "Your solutions to computational questions should include any specified Python code and results \n",
    "as well as written commentary on your conclusions.\n",
    "Remember that you are encouraged to discuss the problems with your instructors and classmates, \n",
    "but **you must write all code and solutions on your own**. For a refresher on the course **Collaboration Policy** click [here](https://canvas.uchicago.edu/courses/42240).\n",
    "\n",
    "**NOTES**: \n",
    "\n",
    "- Do **NOT** load or use any Python packages that are not available in Anaconda with Python 3.9. \n",
    "- Some problems with code may be autograded.  If we provide a function API, **do not** change it.  If we do not provide a function API, then you're free to structure your code however you like. \n",
    "- Submit only this Jupyter notebook to Canvas.  Do not compress it using tar, rar, zip, etc. \n",
    "\n",
    "**Acknowledgment**: Noah Smith, Chris Ketelsen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name**: Yujie Jiang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2817b95d5e16f06bafb80bf0796d64d2",
     "grade": false,
     "grade_id": "load",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "999a59eb16aacdd0d546c483f159c402",
     "grade": false,
     "grade_id": "1intro",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### [60 points] Problem 1 - Decision tree\n",
    "***\n",
    "\n",
    "Consider the problem of predicting whether a person has a college degree based on age, salary, and Chicago residency. \n",
    "The dataset looks like the following.\n",
    "\n",
    "| Age   | Salary         | Chicago Residency      | College degree| \n",
    "|:------:|:------------:| :-----------:|---:|\n",
    "| 27 | 41,000 | Yes | Yes |\n",
    "| 61 | 52,000 | No | No |\n",
    "| 23 | 24,000 | Yes | No |\n",
    "| 29 | 77,000 | Yes | Yes |\n",
    "| 32 | 48,000 | No | Yes |\n",
    "| 57 | 120,000 | Yes | Yes |\n",
    "| 22 | 38,000 | Yes | Yes |\n",
    "| 41 | 45,000 | Yes | No |\n",
    "| 53 | 26,000 | No | No |\n",
    "| 48 | 65,000 | Yes | Yes |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "57ff8a7b88e7fbb838a663145288e261",
     "grade": false,
     "grade_id": "1q",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Part A [5 points]**: Convert the above table to data. Two variables should be created:\n",
    "        \n",
    "1. $x$ is a $10*3$ matrix that contains the data from columns 0, 1, and 2. Chicago residency is represented by 1 (yes) and 0 (no).\n",
    "2. $y$ contains the labels (college degree), 1 (yes) and 0 (no)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "99d49c4279d566943cb0e9e1eed98c4c",
     "grade": true,
     "grade_id": "1a",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([[27, 41000, 1], [61, 52000, 0], [23, 24000, 1], [29, 77000, 1], [32, 48000, 0],\n",
    "     [57, 120000, 1], [22, 38000, 1], [41, 45000, 1], [53, 26000, 0], [48, 65000, 1]])\n",
    "y = np.array([1, 0, 0, 1, 1, 1, 1, 0, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a05258269121b3b60d48adaf038dc676",
     "grade": false,
     "grade_id": "2q1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Part B [10 points]:** Criteria for choosing a feature to split.\n",
    "\n",
    "**[4 points]** We start with no splitting. Assuming that our algorithm is deterministic, what is the smallest number of mistakes we can make if we do not use any of the features and what is the algorithm? (**Write your answer in the Markdown cell below.**)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "483fd69f569c413806fa39b054f7baea",
     "grade": true,
     "grade_id": "2a1",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "The algorithm behind which we don't use any feature is just random guess, thus it is more likely for us to make mistakes and the smallest number of mistakes we can make will be generally high. However, exact value of the smallest number of mistakes should be depends. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2636e94124c66892be090f27550d92c6",
     "grade": false,
     "grade_id": "2q2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**[6 points]** We start by considering the variable *Chicago residency*. The first criteria is based on the number of mistakes. We need to build a contingency table between Chicago residency and college degree.\n",
    "\n",
    "How many mistakes will we make if we split based on Chicago residency? (**Answer below by finishing the code.**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "faa4e220d79ed9e1f956512242efdbd2",
     "grade": false,
     "grade_id": "2a2",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_error_in_leaf(y, ids):\n",
    "    \"\"\"\n",
    "    Returns the errors in a leaf node of a decision tree.\n",
    "    This function can be used to answer the previous question automatically.\n",
    "    \n",
    "    :@param y: all labels\n",
    "    :@param ids: the subset of indexes in the leaf node\n",
    "    \"\"\"\n",
    "    zero_count = 0\n",
    "    one_count = 0\n",
    "    leaf = y[ids]\n",
    "    for label in leaf:\n",
    "        if label == 0:\n",
    "            zero_count += 1\n",
    "        elif label == 1:\n",
    "            one_count += 1\n",
    "    if zero_count > one_count:\n",
    "        return one_count\n",
    "    else:\n",
    "        return zero_count \n",
    "\n",
    "def error_criteria(y, root, left_child, right_child):\n",
    "    \"\"\"\n",
    "    Returns the number of errors if we split the root into the left child and the right child.\n",
    "    \n",
    "    :@param y: all labels\n",
    "    :@param root: indexes of all the data points in the root\n",
    "    :@param left_child: the subset of indexes in the left child\n",
    "    :@param right_child: the subset of indexes in the right child\n",
    "    \"\"\"\n",
    "    left_error = get_error_in_leaf(y, left_child)\n",
    "    right_error = get_error_in_leaf(y, right_child)\n",
    "    return left_error + right_error\n",
    "\n",
    "def value_split_binary_feature(x, y, fid, root, criteria_func):\n",
    "    \n",
    "    root = root.tolist()\n",
    "    left_child = [i for i in root if x[i, fid] == 0]\n",
    "    right_child = [i for i in root if x[i, fid] == 1]\n",
    "    return criteria_func(y, root, left_child, right_child)\n",
    "\n",
    "# Chicago residency should correpsond to the third column in your data x\n",
    "fid = 2\n",
    "root = np.array(list(range(len(y)))) # root includes all data points\n",
    "mistakes = value_split_binary_feature(x, y, fid, root, error_criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6be5f43afda128ee0239b2b3e9bc93ff",
     "grade": true,
     "grade_id": "cell-dd1ff1e66981cc11",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is for grading purposes only; please ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ded6d069a782d1483c34462a10b4a60f",
     "grade": false,
     "grade_id": "2a3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Part C [20 points]** Alternatively, we can use entropy and information gain to split the data. In this part, you will manually determine the first split in a decision tree. Please do not use code in your calculations for part C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1e85886de3472cd141af6569380005b1",
     "grade": false,
     "grade_id": "cell-e5a060788c99dc75",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**[4 points]** To get familiar with MathJax, please write the equations necessary to compute entropy and information gain if we split data $D$ into $D_1$ and $D_2$. **Write your answer in the Markdown cell below.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b17e7d719a579c48373397b74c8f91f1",
     "grade": true,
     "grade_id": "cell-f6f2bbd3029c7b93",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "Entropy: $$H(x) = -p_i *\\log_2 p_i - (1-p_i)*\\log_2 (1-p_i)$$\n",
    "Information Gain: $$IG(D,D_i) = H(D) - \\frac{X_l}{X_D}*H(X_l) - \\frac{X_r}{X_D}*H(X_r)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "26e8b99c69ae0b16d74c1eec8c585c36",
     "grade": false,
     "grade_id": "cell-ff4b2e7471a3a466",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**[4 points]** What is the entropy for College Degree? Please show your work. Were you expecting a result around this number? Why?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f00d4adb7e6c48089bb624820b851270",
     "grade": true,
     "grade_id": "cell-9954ce044843f7c0",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Entropy for College Degree: $$ -\\frac{3}{5} * log_2 \\frac{3}{5} -\\frac{2}{5} * log_2 \\frac{2}{5} \\approx 0.98 $$\n",
    "I exapected it to be a number around it because the College Degree is almost equally balanced, not much infomation can be given. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9d793934e3c2719314e0d5ca3f261580",
     "grade": false,
     "grade_id": "cell-6c10c241a34d72a7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**[4 points]** What is the information gain for College Degree if you split the observations on the Chicago Residency attribute? Please show your work."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7897d6345e22a78e2f84097f9c25e34c",
     "grade": true,
     "grade_id": "cell-b465dc36f5c4b733",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "$$ H(X_l) = -\\frac{1}{3} * log_2 \\frac{1}{3} -\\frac{2}{3} * log_2 \\frac{2}{3} \\approx 0.918 $$\n",
    "\n",
    "$$ H(X_r) = - \\frac{2}{7} * log_2 \\frac{2}{7} -\\frac{5}{7} * log_2 \\frac{5}{7} \\approx 0.863 $$\n",
    "\n",
    "$$ IG = 0.98 - \\frac{3}{10} * 0.918 - \\frac{7}{10} * 0.863 \\approx 0.1005$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "32ca26854e72561a6f71ee150a5680e4",
     "grade": false,
     "grade_id": "cell-2421ce359be01217",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**[4 points]** One way to deal with continuous (or ordinal) features like Age and Salary is to define binary features based on thresholding. For example, you might convert ages to 0 if Age is less than or equal to 50 and 1 otherwise. What is the information gain for College Degree if we split the observations based on the Salary attribute with a threshold of \\$50,000? Please show your work.\n",
    "\n",
    "(Later in this problem, we will write code to determine whether a number other than \\$50,000 might be the optimal threshold for Salary. For now, just assume that a threshold of \\\\$50,000 is optimal.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a1b31913c3305f377ec4d130273967a",
     "grade": true,
     "grade_id": "cell-e8a8ac44a50254a6",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "$$ H(X_l) = -\\frac{1}{2} * log_2 \\frac{1}{2} -\\frac{1}{2} * log_2 \\frac{1}{2} \\approx 0 $$\n",
    "\n",
    "$$ H(X_r) = - \\frac{1}{4} * log_2 \\frac{1}{4} -\\frac{3}{4} * log_2 \\frac{3}{4} \\approx 0.811$$\n",
    "\n",
    "$$ IG = 0.98 - \\frac{3}{5} * 0 - \\frac{2}{5} * 0.811 \\approx 0.656$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aaa204fb0aea2bfdafdba1377a31c07a",
     "grade": false,
     "grade_id": "cell-a98cc6a9b99b8b36",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**[4 points]** Based on the information gain calculations, should the first split in our decision tree be on Chicago Residency or on Salary with a \\$50,000 threshold? Is this the answer you expected based on the data counts? Why or why not?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "10707aa247b7a01d06f3200326ba16c0",
     "grade": true,
     "grade_id": "cell-06c07fbc2fc63a2a",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "We should firstly split our decision tree by Salary with a $50,000 threshold, because it generates higher information gain. \n",
    "It is not as expected, it is hard to tell which one is better if only based on the data counts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7266539b6061dc3ba06ce9fc40acc432",
     "grade": false,
     "grade_id": "cell-b90188ff50c2eaf9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Part D [10 points]** Now we will write a function for computing information gain. Use log2 for entropy computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a112f7e33530182c36e6fc39e8b74421",
     "grade": false,
     "grade_id": "cell-bb989c485308fcc4",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def entropy(y, ids):\n",
    "    \"\"\"\n",
    "    Returns the entropy in the labels for the data points in ids.\n",
    "    \n",
    "    :@param y: all labels\n",
    "    :@param ids: the indexes of data points\n",
    "    \"\"\"\n",
    "    zero_count = 0\n",
    "    one_count= 0\n",
    "    leaf = y[ids]\n",
    "\n",
    "    if len(ids) == 0: # deal with corner case when there is no data point.\n",
    "        return 0\n",
    "    for label in leaf:\n",
    "        if label == 0:\n",
    "            zero_count += 1 \n",
    "        else:\n",
    "            one_count += 1\n",
    "    \n",
    "    zero_prob = zero_count/len(ids)\n",
    "    one_prob = one_count/len(ids)\n",
    "    entropy = -zero_prob * math.log2(zero_prob) - one_prob * math.log2(one_prob)\n",
    "    return entropy\n",
    "    \n",
    "def information_gain_criteria(y, root, left_child, right_child):\n",
    "    \"\"\"\n",
    "    Returns the information gain by splitting root into left child and right child.\n",
    "    \n",
    "    :@param y: all labels\n",
    "    :@param root: indexes of all the data points in the root\n",
    "    :@param left_child: the subset of indexes in the left child\n",
    "    :@param right_child: the subset of indexes in the right child\n",
    "    \"\"\"\n",
    "    left_div = len(left_child)/len(root)\n",
    "    right_div = len(right_child)/len(root)\n",
    "    info_gain = entropy(y, root) - left_div * entropy(y, left_child) - right_div * entropy(y, right_child)\n",
    "    return info_gain\n",
    "    \n",
    "fid = 2\n",
    "root = np.array(list(range(len(y)))) # root includes all data points\n",
    "info_gain = value_split_binary_feature(x, y, fid, root, information_gain_criteria)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e69b435c5de4b7b8f56db1d1d6c5ab1",
     "grade": true,
     "grade_id": "cell-1059c1f151862da1",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is for grading purposes only; please ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cbcb9570e6845117b277658fc4bc30c9",
     "grade": false,
     "grade_id": "cell-7a340274d5eccbf2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Part E [15 points]**: Deal with continuous features.\n",
    "    \n",
    "**[10 points]** Complete the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "64363b21761e99525c42238eb1d677a3",
     "grade": false,
     "grade_id": "cell-94945cb8ee7f6b14",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def value_split_continuous_feature(x, y, fid, root, criteria_func=information_gain_criteria):\n",
    "    \"\"\"\n",
    "    Return the best value and its corresponding threshold by splitting based on a continuous feature.\n",
    "\n",
    "    :@param x: all feature values\n",
    "    :@param y: all labels\n",
    "    :@param fid: feature id to split the tree based on\n",
    "    :@param root: indexes of all the data points in the root\n",
    "    :@param criteria_func: the splitting criteria function\n",
    "    \"\"\"\n",
    "    best_value, best_thres = 0, 0\n",
    "\n",
    "    return best_value, best_thres\n",
    "\n",
    "root = np.array(range(len(y))) # root includes all data points\n",
    "fid = 0\n",
    "age_value, age_thres = value_split_continuous_feature(x, y, fid, root, information_gain_criteria)\n",
    "fid = 1\n",
    "salary_value, salary_thres = value_split_continuous_feature(x, y, fid, root, information_gain_criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6c7419eb6634436491116afc5b2fe913",
     "grade": true,
     "grade_id": "cell-b073fa94cd720ea6",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is for grading purposes only; please ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f9465ed51fccf241b82e00ba4e593650",
     "grade": true,
     "grade_id": "cell-4fca1b5988b02f7a",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is for grading purposes only; please ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "efec5b53ab6827b916bc478118b3d9a8",
     "grade": false,
     "grade_id": "cell-c7507cb413cfd74c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**[5 points]** Based on the current information gain by splitting different features, if we build a decision stump (decision tree with depth 1) greedily, which feature should we choose? Why? **Write down your answer in the Markdown cell below.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c4202e9f4e5a2074deb181ae4002b6d3",
     "grade": true,
     "grade_id": "cell-db1dde1ecdde83ee",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9e746c872c2d4569bea86e57d75a58e7",
     "grade": false,
     "grade_id": "q1-extra-credit",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Extra credit [5 points]**: You now have all the ingredients to build a decision tree recursively. You can build a decision tree of depth two and report its classification error on the training data and the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af1dff830e3ca2ac0dde968735b78ae2",
     "grade": false,
     "grade_id": "a1-extra-credit",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class LeafNode:\n",
    "    \"\"\"\n",
    "    Class for leaf nodes in the decision tree\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, label, count, total):\n",
    "        \"\"\"\n",
    "        :@param label: label of the leaf node\n",
    "        :@param count: number of data points with class 'label' falling in this leaf\n",
    "        :@param count: number of datapoints of any label falling in this leaf\n",
    "        \"\"\"\n",
    "        self.label = label\n",
    "        self.count = count\n",
    "        self.total = total\n",
    "        \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Return predictions for features x\n",
    "\n",
    "        :@param x: feature values\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def display(self, feat_names, out_str, depth=0):\n",
    "        \"\"\"\n",
    "        Display contents of a leaf node\n",
    "        \"\"\"\n",
    "        prefix = '\\t'*depth\n",
    "        error = 1.0 - self.count / float(self.total)\n",
    "        out_str += f'{prefix}leaf: label={self.label}, error={error} ({self.count}/{self.total} correct)\\n'\n",
    "        return out_str\n",
    "    \n",
    "class TreeNode:\n",
    "    \"\"\"\n",
    "    Class for internal (non-leaf) nodes in the decision tree\n",
    "    \"\"\"\n",
    "    def __init__(self, feat_id, feat_val):\n",
    "        \"\"\"\n",
    "        :@param feat_id: index of the feature that this node splits on\n",
    "        :@param feat_val: threshold for the feature that this node splits on\n",
    "        \"\"\"\n",
    "        self.feat_id = feat_id\n",
    "        self.feat_val = feat_val\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "    \n",
    "    def split(self, x, root):\n",
    "        \"\"\"\n",
    "        Given the datapoints falling into current node, return two arrays of indices in x corresponding to the\n",
    "        left and right subtree\n",
    "        \n",
    "        :@param x: all feature values\n",
    "        :@param root: indexes of all the data points in the current node\n",
    "        \"\"\"\n",
    "        root = np.array(root)\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Return an array of predictions for given 'x' for the current node\n",
    "        \n",
    "        :@param x: datapoints\n",
    "        \"\"\"\n",
    "        assert self.left is not None and self.right is not None, 'predict called before fit'\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def display(self, feat_names, out_str, depth=0):\n",
    "        \"\"\"\n",
    "        Display contents of a non-leaf node\n",
    "        \"\"\"\n",
    "        prefix = '\\t'*depth\n",
    "        out_str += f'{prefix}{feat_names[self.feat_id]}\\n'\n",
    "        out_str += f'{prefix}x <= {self.feat_val}\\n'\n",
    "        out_str = self.left.display(feat_names, out_str, depth=depth+1)\n",
    "        out_str += f'{prefix}x > {self.feat_val}\\n'\n",
    "        out_str = self.right.display(feat_names, out_str, depth=depth+1)\n",
    "        return out_str\n",
    "\n",
    "class DecisionTree:\n",
    "    \"\"\"\n",
    "    Class for the decision tree\n",
    "    \"\"\"\n",
    "    def __init__(self, max_depth=1, criteria_func=information_gain_criteria, binary_feat_ids=[]):\n",
    "        \"\"\"\n",
    "        :@param max_depth: Maximum depth that a decision tree can take\n",
    "        :@param criteria_func: criteria function to split features\n",
    "        :@param binary_feat_id: list of indexes of binary features\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.criteria_func = criteria_func\n",
    "        self.binary_feat_ids = binary_feat_ids\n",
    "        self.root = None\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        \"\"\"\n",
    "        Fit a tree to the given dataset using a helper function\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.root = self.fit_helper(np.array(list(range(self.x.shape[0]))))\n",
    "    \n",
    "    def fit_helper(self, root, depth=1):\n",
    "        \"\"\"\n",
    "        Recursive helper function for fitting a decision tree\n",
    "        Returns a node (can be either LeafNode or TreeNode)\n",
    "        \n",
    "        :@param root: array of indices of datapoints which fall into the current node\n",
    "        :@param depth: current depth of the tree being built \n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        Strategy:\n",
    "        1. If current partition is pure i.e. labels corresponding to all indices in root are the same\n",
    "           OR the maximum depth has been reached, stop building the tree and return a LeafNode\n",
    "        2. If not, find out the best feature to split on along with the threshold, create a TreeNode and \n",
    "           recursively call fit_helper on the two splits (You can assume the threshold for a binary feature \n",
    "           to be 0.5). Finally, return the current node \n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Return predictions for a given dataset  \n",
    "        \"\"\"\n",
    "        assert self.root is not None, 'fit not yet called'\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    \n",
    "    def display(self, feat_names):\n",
    "        assert self.root is not None, 'fit not yet called'\n",
    "        out_str = \"\"\n",
    "        out_str = self.root.display(feat_names, out_str)\n",
    "        return out_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "22fc2d2915109be0a17a61e7b488d418",
     "grade": true,
     "grade_id": "cell-24ff429d1f9a70fc",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is for grading purposes only; please ignore\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f46d05b143e83eb978885711367e74ae",
     "grade": false,
     "grade_id": "cell-44230b1deea50132",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### [40 points] Problem 2 - KNN for Crime Prediction \n",
    "***\n",
    "\n",
    "In this problem, you will implement a K-Nearest Neighbors framework to predict the type of crime reported based on an incident's location.\n",
    "\n",
    "Download [reported crime data for 2021](https://data.cityofchicago.org/Public-Safety/Crimes-2021/dwme-t96c) from the Chicago Open Data Portal (we already prepared it in the data folder, but would you to know where the source was).\n",
    "\n",
    "**Part A [5 points]**: Load the data.  We will only use three columns of the dataset: Primary Type, Latitude, and Longitude.\n",
    "- Be sure to drop any observations that are missing latitude and/or longitude.\n",
    "- To reduce run time, only keep incidents reported as one of the the four most common crime types ('THEFT', 'BATTERY', 'CRIMINAL DAMAGE', 'ASSAULT'). \n",
    "- Use the train_test_split function from Scikit-Learn to split the data into training and validation sets.  Set random_state=123 and test_size=0.2.\n",
    "- Finally, explore the training and validation sets and answer the following questions: \n",
    "  - How many total observations are in the training set? \n",
    "  - How many total observations are in the validation set? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d7fde237a9ed9fec31582dc1c58abd8e",
     "grade": false,
     "grade_id": "cell-50674b47f654cd5f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m N_validation_examples \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# YOUR CODE HERE\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m()\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Write code for answering the questions in Part A and then put your answer in the Markdown cell below.\n",
    "# Make sure to set each of the variables below to the correct value. Do not rename the variables.\n",
    "N_training_examples = None\n",
    "N_validation_examples = None\n",
    "\n",
    "crime_data = pd.read_csv(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2437cf652e9550b85a5e1c3492b28872",
     "grade": true,
     "grade_id": "cell-4b99d8373143f879",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is for grading purposes only; please ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3b50c3b1db910c49ecb8650de370f042",
     "grade": true,
     "grade_id": "cell-738b21d7f854fdb0",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fd333f2bded36de18f698df689ccc32b",
     "grade": false,
     "grade_id": "cell-72e3e9dab28970af",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Optional:**  If you'd like to run the following code chunk to render a map of the incidents, you will need to download the [neighborhood boundaries data](https://data.cityofchicago.org/Facilities-Geographic-Boundaries/Boundaries-Neighborhoods/bbvz-uum9) from the Chicago Data Portal (again, we have provided a version in the data folder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import geopandas as gpd\n",
    "\n",
    "# Load boundaries\n",
    "file = open('data/boundaries.geojson')\n",
    "boundaries = gpd.read_file(file) \n",
    "\n",
    "# Convert full crime dataset from pandas to geopandas\n",
    "crime_gdf = gpd.GeoDataFrame(crime_df, geometry=gpd.points_from_xy(crime_df['Longitude'], crime_df['Latitude']),\n",
    "                             crs='EPSG:4326')\n",
    "\n",
    "# Make plot\n",
    "fig, ax = plt.subplots(1, figsize=(12, 16))\n",
    "boundaries.plot(ax=ax, color='white', edgecolor='black')\n",
    "colors = {'THEFT' : 'red', 'BATTERY' : 'blue', 'CRIMINAL DAMAGE' : 'yellow', 'ASSAULT' : 'gray'}\n",
    "grouped = crime_gdf.groupby('Primary Type', sort=True)\n",
    "for key in colors:\n",
    "    group = grouped.get_group(key)\n",
    "    group.plot(ax=ax, label=key, color=colors[key], markersize=0.3)\n",
    "ax.legend(markerscale=10)\n",
    "ax.axis('off')\n",
    "ax.set_title('Reported Incidents in Chicago, 2021', fontdict={'fontsize': '25', 'fontweight' : '3'});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5a91d26641bc838c1c8af25187c0ab84",
     "grade": false,
     "grade_id": "cell-ea6def68186d971b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class KNN:\n",
    "    \"\"\"\n",
    "    Class to store data for regression problems \n",
    "    \"\"\"\n",
    "    def __init__(self, X_train, y_train, K=5, distance_weighted=False):\n",
    "        \"\"\"\n",
    "        Creates a kNN instance\n",
    "\n",
    "        :param X_train: Training data input in 2D ndarray \n",
    "        :param y_train: Training data output in 1D ndarray \n",
    "        :param K: The number of nearest points to consider in classification\n",
    "        :param distance_weighted: Bool indicating whether to use distance weighting\n",
    "        \"\"\"\n",
    "        \n",
    "        # Import and build the BallTree on training features \n",
    "        from sklearn.neighbors import BallTree\n",
    "        self.balltree = BallTree(X_train)\n",
    "        \n",
    "        # Cache training labels and parameter K \n",
    "        self.y_train = y_train\n",
    "        self.K = K \n",
    "        \n",
    "        # Boolean flag indicating whether to do distance weighting \n",
    "        self.distance_weighted = distance_weighted\n",
    "        \n",
    "    def majority(self, neighbor_indices, neighbor_distances=None):\n",
    "        \"\"\"\n",
    "        Given indices of nearest neighbors in training set, return the majority label. \n",
    "        Break ties by considering 1 fewer neighbor until a clear winner is found. \n",
    "\n",
    "        :param neighbor_indices: The indices of the K nearest neighbors in self.X_train \n",
    "        :param neighbor_distances: Corresponding distances from query point to K nearest neighbors. \n",
    "        \"\"\"\n",
    "        \n",
    "        # If only one neighbor, return label of that neighbor \n",
    "        if len(neighbor_indices) == 1:\n",
    "            return self.y_train[neighbor_indices[0]]\n",
    "        \n",
    "        # If no distances provided, set to ones \n",
    "        if neighbor_distances is None:\n",
    "            neighbor_distances = np.ones(len(neighbor_indices))\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "            \n",
    "    def classify(self, x):\n",
    "        \"\"\"\n",
    "        Given a query point, return the predicted label \n",
    "        \n",
    "        :param x: a query point stored as an ndarray  \n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Given an ndarray of query points, return yhat, an ndarray of predictions \n",
    "\n",
    "        :param X: an (m x p) dimension ndarray of points to predict labels for \n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e5be9bca39d224fe5d9a82eabdb3691e",
     "grade": false,
     "grade_id": "cell-7f873552cc36e87a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Part B [10 points]**: Modify the class above to implement an Unweighted KNN classifier.  There are three methods that you need to complete: \n",
    "\n",
    "- `predict`: Given an $m \\times p$ matrix of validation data with $m$ examples each with $p$ features, return a length-$m$ vector of predicted labels by calling the `classify` function on each example. \n",
    "- `classify`: Given a single query example with $p$ features, return its predicted class label as a string using KNN by calling the `majority` function. \n",
    "- `majority`: Given an array of indices into the training set corresponding to the $K$ training examples that are nearest to the query point, return the majority label as a string.  If there is a tie for the majority label using $K$ nearest neighbors, reduce $K$ by 1 and try again.  Continue reducing $K$ until there is a winning label. \n",
    "\n",
    "**Notes**: \n",
    "- Don't even think about implementing nearest-neighbor search or any distance metrics yourself.  Instead, go read the documentation for Scikit-Learn's [BallTree](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html) object.  You will find that its implemented [query](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree.query) method can do most of the heavy lifting for you. \n",
    "- **Do not** use Scikit-Learn's KNeighborsClassifier in this problem.  We're implementing this ourselves. \n",
    "- You don't need to worry about the `distance_weighted` flag until **Part C**, but we recommend reading ahead a bit. It might be good to think about your implementation of **Part C** before implementing **Part B**. \n",
    "- When you think you're done, execute the following cell to run 4 unit tests based on the example starting on Slide 15 of the KNN Lecture slides on Canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5ef6933cca19f90612c0b56ad1ce7820",
     "grade": true,
     "grade_id": "cell-798d57fabb83fe1b",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from tests import tests\n",
    "tests.run_test_suite('prob 2A', KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "533db6ac372cf138271264257c3aa184",
     "grade": false,
     "grade_id": "cell-ea5a98439cad3d65",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Part C [5 points]**: Modify the `KNN` class to perform the distance-weighted KNN classification.\n",
    "The so-called Distance-Weighted KNN classifier assigns weights to the nearest-neighbor training examples proportional to the inverse-distance from the training example to the query point.  Classification is performed by summing the weights associated with each class and predicting the class with the highest weighted-majority vote.  Mathematically we might describe the weighted-vote for a class $c$ as \n",
    "\n",
    "$$\n",
    "\\textrm{Weighted-Vote}(c) = \\displaystyle\\sum_{i \\in {\\cal N}_K} I(y_i = c) \\times \\dfrac{1}{\\|{\\bf x}_i - {\\bf x}\\|}\n",
    "$$\n",
    "\n",
    "A word of caution: it's certainly possible that a query point could be distance $0$ away from some training example.  If this happens your implementation should handle it gracefully and return the appropriate class label.   \n",
    "\n",
    "When you think you're done, execute the following cell to run three final unit tests corresponding to the example on Slide 21 of the KNN Lecture slides on Canvas. Make sure that the changes you make in **Part C** do not affect the unit tests from **Part B**.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ee661642d97a144e6c608a9f3b81d60d",
     "grade": true,
     "grade_id": "cell-3949b9f6b901e47a",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from tests import tests\n",
    "tests.run_test_suite('prob 2B', KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5c0ed8a177f54bbba25e8706b88861ac",
     "grade": false,
     "grade_id": "cell-0da0a9554edce07f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Part D [8 points]**: Use your `KNN` class to perform weighted KNN on the validation data with $K=100$ and do the following: \n",
    "\n",
    "- **[4 points]** Create a **confusion matrix** (feel free to use the Scikit-Learn [confusion_matrix](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) function).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "761b342712dc6a04902a4d204a6ff441",
     "grade": true,
     "grade_id": "cell-4bdb9c57c0f418e2",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "knn = KNN(X_train, y_train, K=100, distance_weighted=True)\n",
    "yhat_valid = knn.predict(X_valid)\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "88ccfcb8113150e2cc873e17d1174c5b",
     "grade": false,
     "grade_id": "cell-2b9ba29fce61478c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "- **[4 points]** Based on your confusion matrix, which crime types are most frequently misclassified as other crime types? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1c1db39e97784b55e28384ad058367b9",
     "grade": true,
     "grade_id": "cell-796deb7245c9b7b7",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part E [12 points]**: **[4 points]** Create a plot of the accuracy of both Unweighted and Distance-Weighted KNN on the validation set on the same set of axes for various values of $K$. Your plot should show the approximate value of $K$ that maximizes the accuracy. Note that accuracy will be below 50%, but above 25% (i.e., better than random classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e0df7ab0421cea8626a4b6f21aee428b",
     "grade": true,
     "grade_id": "cell-173f9cb33d3db963",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "acc = []\n",
    "wacc = []\n",
    "allks = [50, 100, 150, 200, 250, 300]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "    \n",
    "fig, ax = plt.subplots(nrows=1,ncols=1,figsize=(12, 7))\n",
    "ax.plot(allks, acc, marker=\"o\", color=\"steelblue\", lw=3, label=\"unweighted\")\n",
    "ax.plot(allks, wacc, marker=\"o\", color=\"green\", lw=3, label=\"weighted\")\n",
    "ax.set_xlabel(\"number neighbors\", fontsize=16)\n",
    "ax.set_ylabel(\"accuracy\", fontsize=16)\n",
    "ax.legend(loc=\"upper right\")\n",
    "plt.xticks(range(50, 301, 50))\n",
    "ax.grid(alpha=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7ae52761146cb44f609ab681ee91ee09",
     "grade": false,
     "grade_id": "cell-fc4bc58f96c7bf69",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**[4 points]** Based on the plot, answer the following questions: \n",
    "\n",
    "- For general $K$, does Unweighted or Weighted KNN appear to perform better? \n",
    "- Which value of $K$ attains the best accuracy on the validation set? \n",
    "\n",
    "Open questions: Why do you think this is the case? How can you explain this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ac6ab6c8a964f983bd671196cf21e8f1",
     "grade": true,
     "grade_id": "cell-0f870649fa5e1c29",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2d23587656e82348e5852d1381cd314d",
     "grade": false,
     "grade_id": "cell-76c5bd43c451bc71",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**[4 points]** If you had unrestricted access to data, how might you improve the model to try to increase classification accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "edde026b64374dd536303a5b9501c3e5",
     "grade": true,
     "grade_id": "cell-7ee1e188129c22ad",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3d6cfe1a807ce7839a8d89c76f278223",
     "grade": true,
     "grade_id": "cell-bda8fadfb14fc199",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e6c3734406936d31b885d236ce616006",
     "grade": false,
     "grade_id": "coursesurvey",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Optional survey.\n",
    "***\n",
    "\n",
    "We are always interested in your feedback. At the end of each homework, there is a simple anonymous feedback [survey](https://docs.google.com/forms/d/e/1FAIpQLScOhZRze8OcoWVgOf-FP5gJFiecJPVFkGnQSfC2zGrvUmHdrQ/viewform?usp=sf_link) to solicit your feedback for how to improve the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "9d62e3916d1e29f0583345cd9ea837eeb4d330fd9bef957a4b9ce042a25fde1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
