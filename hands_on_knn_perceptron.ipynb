{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On KNN and Perceptron\n",
    "***\n",
    "\n",
    "In this notebook we'll investigate Scikit-Learn's implementation of K-Nearest Neighbors and the Perceptron classifier.  In addition, we'll look at how we can evaluate the performance our classifiers with a so-called confusion matrix.  \n",
    "\n",
    "**Note**: There are some helper functions at the bottom of this notebook.  Scroll down and execute those cells before continuing. \n",
    "\n",
    "**Acknowledgment**: Chris Ketelsen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Classifying Iris Species \n",
    "***\n",
    "\n",
    "In this problem we'll use K-Nearest Neighbors to classify species of irises based on certain physical characteristics.  The so-called [_iris dataset_](https://en.wikipedia.org/wiki/Iris_flower_data_set) is a popular dataset for prototyping classification algorithms. We can load the iris dataset from Scikit-Learn directly. The dataset contains four features: sepal length, sepal width, pedal length, and pedal width and three classes defined by the species of iris: setosa, versicolor, and virginica. We'll only use the sepal dimensions so that we can easily visualize the data. \n",
    "\n",
    "Execute the following code cell to load training and validation sets for the iris data set and then plot the data.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
      "       [4.9, 3. , 1.4, 0.2],\n",
      "       [4.7, 3.2, 1.3, 0.2],\n",
      "       [4.6, 3.1, 1.5, 0.2],\n",
      "       [5. , 3.6, 1.4, 0.2],\n",
      "       [5.4, 3.9, 1.7, 0.4],\n",
      "       [4.6, 3.4, 1.4, 0.3],\n",
      "       [5. , 3.4, 1.5, 0.2],\n",
      "       [4.4, 2.9, 1.4, 0.2],\n",
      "       [4.9, 3.1, 1.5, 0.1],\n",
      "       [5.4, 3.7, 1.5, 0.2],\n",
      "       [4.8, 3.4, 1.6, 0.2],\n",
      "       [4.8, 3. , 1.4, 0.1],\n",
      "       [4.3, 3. , 1.1, 0.1],\n",
      "       [5.8, 4. , 1.2, 0.2],\n",
      "       [5.7, 4.4, 1.5, 0.4],\n",
      "       [5.4, 3.9, 1.3, 0.4],\n",
      "       [5.1, 3.5, 1.4, 0.3],\n",
      "       [5.7, 3.8, 1.7, 0.3],\n",
      "       [5.1, 3.8, 1.5, 0.3],\n",
      "       [5.4, 3.4, 1.7, 0.2],\n",
      "       [5.1, 3.7, 1.5, 0.4],\n",
      "       [4.6, 3.6, 1. , 0.2],\n",
      "       [5.1, 3.3, 1.7, 0.5],\n",
      "       [4.8, 3.4, 1.9, 0.2],\n",
      "       [5. , 3. , 1.6, 0.2],\n",
      "       [5. , 3.4, 1.6, 0.4],\n",
      "       [5.2, 3.5, 1.5, 0.2],\n",
      "       [5.2, 3.4, 1.4, 0.2],\n",
      "       [4.7, 3.2, 1.6, 0.2],\n",
      "       [4.8, 3.1, 1.6, 0.2],\n",
      "       [5.4, 3.4, 1.5, 0.4],\n",
      "       [5.2, 4.1, 1.5, 0.1],\n",
      "       [5.5, 4.2, 1.4, 0.2],\n",
      "       [4.9, 3.1, 1.5, 0.2],\n",
      "       [5. , 3.2, 1.2, 0.2],\n",
      "       [5.5, 3.5, 1.3, 0.2],\n",
      "       [4.9, 3.6, 1.4, 0.1],\n",
      "       [4.4, 3. , 1.3, 0.2],\n",
      "       [5.1, 3.4, 1.5, 0.2],\n",
      "       [5. , 3.5, 1.3, 0.3],\n",
      "       [4.5, 2.3, 1.3, 0.3],\n",
      "       [4.4, 3.2, 1.3, 0.2],\n",
      "       [5. , 3.5, 1.6, 0.6],\n",
      "       [5.1, 3.8, 1.9, 0.4],\n",
      "       [4.8, 3. , 1.4, 0.3],\n",
      "       [5.1, 3.8, 1.6, 0.2],\n",
      "       [4.6, 3.2, 1.4, 0.2],\n",
      "       [5.3, 3.7, 1.5, 0.2],\n",
      "       [5. , 3.3, 1.4, 0.2],\n",
      "       [7. , 3.2, 4.7, 1.4],\n",
      "       [6.4, 3.2, 4.5, 1.5],\n",
      "       [6.9, 3.1, 4.9, 1.5],\n",
      "       [5.5, 2.3, 4. , 1.3],\n",
      "       [6.5, 2.8, 4.6, 1.5],\n",
      "       [5.7, 2.8, 4.5, 1.3],\n",
      "       [6.3, 3.3, 4.7, 1.6],\n",
      "       [4.9, 2.4, 3.3, 1. ],\n",
      "       [6.6, 2.9, 4.6, 1.3],\n",
      "       [5.2, 2.7, 3.9, 1.4],\n",
      "       [5. , 2. , 3.5, 1. ],\n",
      "       [5.9, 3. , 4.2, 1.5],\n",
      "       [6. , 2.2, 4. , 1. ],\n",
      "       [6.1, 2.9, 4.7, 1.4],\n",
      "       [5.6, 2.9, 3.6, 1.3],\n",
      "       [6.7, 3.1, 4.4, 1.4],\n",
      "       [5.6, 3. , 4.5, 1.5],\n",
      "       [5.8, 2.7, 4.1, 1. ],\n",
      "       [6.2, 2.2, 4.5, 1.5],\n",
      "       [5.6, 2.5, 3.9, 1.1],\n",
      "       [5.9, 3.2, 4.8, 1.8],\n",
      "       [6.1, 2.8, 4. , 1.3],\n",
      "       [6.3, 2.5, 4.9, 1.5],\n",
      "       [6.1, 2.8, 4.7, 1.2],\n",
      "       [6.4, 2.9, 4.3, 1.3],\n",
      "       [6.6, 3. , 4.4, 1.4],\n",
      "       [6.8, 2.8, 4.8, 1.4],\n",
      "       [6.7, 3. , 5. , 1.7],\n",
      "       [6. , 2.9, 4.5, 1.5],\n",
      "       [5.7, 2.6, 3.5, 1. ],\n",
      "       [5.5, 2.4, 3.8, 1.1],\n",
      "       [5.5, 2.4, 3.7, 1. ],\n",
      "       [5.8, 2.7, 3.9, 1.2],\n",
      "       [6. , 2.7, 5.1, 1.6],\n",
      "       [5.4, 3. , 4.5, 1.5],\n",
      "       [6. , 3.4, 4.5, 1.6],\n",
      "       [6.7, 3.1, 4.7, 1.5],\n",
      "       [6.3, 2.3, 4.4, 1.3],\n",
      "       [5.6, 3. , 4.1, 1.3],\n",
      "       [5.5, 2.5, 4. , 1.3],\n",
      "       [5.5, 2.6, 4.4, 1.2],\n",
      "       [6.1, 3. , 4.6, 1.4],\n",
      "       [5.8, 2.6, 4. , 1.2],\n",
      "       [5. , 2.3, 3.3, 1. ],\n",
      "       [5.6, 2.7, 4.2, 1.3],\n",
      "       [5.7, 3. , 4.2, 1.2],\n",
      "       [5.7, 2.9, 4.2, 1.3],\n",
      "       [6.2, 2.9, 4.3, 1.3],\n",
      "       [5.1, 2.5, 3. , 1.1],\n",
      "       [5.7, 2.8, 4.1, 1.3],\n",
      "       [6.3, 3.3, 6. , 2.5],\n",
      "       [5.8, 2.7, 5.1, 1.9],\n",
      "       [7.1, 3. , 5.9, 2.1],\n",
      "       [6.3, 2.9, 5.6, 1.8],\n",
      "       [6.5, 3. , 5.8, 2.2],\n",
      "       [7.6, 3. , 6.6, 2.1],\n",
      "       [4.9, 2.5, 4.5, 1.7],\n",
      "       [7.3, 2.9, 6.3, 1.8],\n",
      "       [6.7, 2.5, 5.8, 1.8],\n",
      "       [7.2, 3.6, 6.1, 2.5],\n",
      "       [6.5, 3.2, 5.1, 2. ],\n",
      "       [6.4, 2.7, 5.3, 1.9],\n",
      "       [6.8, 3. , 5.5, 2.1],\n",
      "       [5.7, 2.5, 5. , 2. ],\n",
      "       [5.8, 2.8, 5.1, 2.4],\n",
      "       [6.4, 3.2, 5.3, 2.3],\n",
      "       [6.5, 3. , 5.5, 1.8],\n",
      "       [7.7, 3.8, 6.7, 2.2],\n",
      "       [7.7, 2.6, 6.9, 2.3],\n",
      "       [6. , 2.2, 5. , 1.5],\n",
      "       [6.9, 3.2, 5.7, 2.3],\n",
      "       [5.6, 2.8, 4.9, 2. ],\n",
      "       [7.7, 2.8, 6.7, 2. ],\n",
      "       [6.3, 2.7, 4.9, 1.8],\n",
      "       [6.7, 3.3, 5.7, 2.1],\n",
      "       [7.2, 3.2, 6. , 1.8],\n",
      "       [6.2, 2.8, 4.8, 1.8],\n",
      "       [6.1, 3. , 4.9, 1.8],\n",
      "       [6.4, 2.8, 5.6, 2.1],\n",
      "       [7.2, 3. , 5.8, 1.6],\n",
      "       [7.4, 2.8, 6.1, 1.9],\n",
      "       [7.9, 3.8, 6.4, 2. ],\n",
      "       [6.4, 2.8, 5.6, 2.2],\n",
      "       [6.3, 2.8, 5.1, 1.5],\n",
      "       [6.1, 2.6, 5.6, 1.4],\n",
      "       [7.7, 3. , 6.1, 2.3],\n",
      "       [6.3, 3.4, 5.6, 2.4],\n",
      "       [6.4, 3.1, 5.5, 1.8],\n",
      "       [6. , 3. , 4.8, 1.8],\n",
      "       [6.9, 3.1, 5.4, 2.1],\n",
      "       [6.7, 3.1, 5.6, 2.4],\n",
      "       [6.9, 3.1, 5.1, 2.3],\n",
      "       [5.8, 2.7, 5.1, 1.9],\n",
      "       [6.8, 3.2, 5.9, 2.3],\n",
      "       [6.7, 3.3, 5.7, 2.5],\n",
      "       [6.7, 3. , 5.2, 2.3],\n",
      "       [6.3, 2.5, 5. , 1.9],\n",
      "       [6.5, 3. , 5.2, 2. ],\n",
      "       [6.2, 3.4, 5.4, 2.3],\n",
      "       [5.9, 3. , 5.1, 1.8]]), 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), 'frame': None, 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'), 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...', 'feature_names': ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'], 'filename': 'iris.csv', 'data_module': 'sklearn.datasets.data'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "X_train, y_train, X_valid, y_valid, target_names = load_iris()\n",
    "print(\"classes = \", target_names)\n",
    "plot_iris(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: How many examples are in the training set?  How many examples belong to each of the three classes? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# TODO\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mNumber of training examples\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlen\u001b[39m(y_train))\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcollections\u001b[39;00m \u001b[39mimport\u001b[39;00m Counter\n\u001b[1;32m      4\u001b[0m num_per_class \u001b[39m=\u001b[39m Counter(y_train)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "print(\"Number of training examples\", len(y_train))\n",
    "from collections import Counter\n",
    "num_per_class = Counter(y_train)\n",
    "print(num_per_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Next we'll train a KNN classifier to predict iris species based on the sepal measurement features.  The KNN classifier in Scikit-Learn is called [KNeighborsClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html).  Go now and check out the documentation. Define and fit a model with $K=15$ to the training set.  The `plot_knn_boundary` function will then plot the KNN decision boundary against the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = # TODO \n",
    "plot_knn_boundary(X_train, y_train, knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Play with the value of $K$ above.  How does the character of the decision boundary change with $K$? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "plot_knn_boundary(X_train, y_train, knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Until this point we've been plotting the KNN decision boundary against the training data, but really we're interested in how our model does on the validation set.  The following code will train a 1-NN classifier and plot the decision boundary against the validation data. How many points in total are misclassified?  Which species get confused with each other the most? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(1).fit(X_train, y_train)\n",
    "plot_knn_boundary(X_valid, y_valid, knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part E**: Counting misclassified points becomes much more difficult when our data sets are very large.  One convenient method for analyzing misclassification is by constructing the so-called confusion matrix. The confusion matrix is `(# classes)` $\\times$ `(# classes)` matrix such that the entry $C_{ij}$ is the number of examples with _true_ label $i$ predicted to have label $j$. \n",
    "\n",
    "We can compute the confusion matrix using Scikit-Learn's [confusion_matrix](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) function. Read the documentation and then fill in the missing code to compute the confusion matrix for the validation data and the 1-NN classifier.  Do the entries in $C$ agree (roughly) with the visual counts you made above? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_hat_valid = knn.predict(X_valid.values)\n",
    "C = confusion_matrix(y_valid.values, y_hat_valid)\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part F**: Vary the number of nearest neighbors used in KNN above and recompute the confusion matrix.  Describe your results. Does there seem to be a particular setting that works better than the others for the validation data ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part G**: Fill in the code below to compute the error rate on the validation data from the confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_rate(C):\n",
    "    # TODO \n",
    "    return 0 \n",
    "\n",
    "print(\"error rate = {}\".format(error_rate(C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: The Perceptron on Simulated Data\n",
    "***\n",
    "\n",
    "In this problem you'll fit a perceptron model to linearly separable data as well as mildly not-linearly separable data. Execute the following cell to load and plot the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = poly_data(100, sep=0.05, rot=np.pi/6)\n",
    "fig, ax = data_plot(scatter=[(X, y)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: Our first task will be to fit a perceptron model to the data using Scikit-Learn's [Perceptron](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html) classifier.  Go now and look at the documentation. Then fit a model using only a single pass of the Perceptron Algorithm. What values for the weights and the bias did the algorithm find? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "perc = Perceptron(max_iter=1, alpha=0.0, shuffle=False)\n",
    "perc.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Plot the resulting perceptron decision boundary by filling in the below. Was the perceptron able to perfectly fit the training data with just a single epoch? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = data_plot(scatter=[(X, y)])\n",
    "xplot = np.linspace(-1.5, 1.5, 20)\n",
    "w, b = perc.coef_[0], perc.intercept_[0]\n",
    "yplot = # TODO \n",
    "ax.plot(xplot, yplot, lw=3, color=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Next we're going to augment the data set so that it's no longer linearly separable.  Execute the following cell to see the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = poly_data(100, sep=0.05, rot=np.pi/6)\n",
    "X_new = np.concatenate((X, np.array([[0.5, 0.5]])))\n",
    "y_new = np.concatenate((y, np.array([-1])))\n",
    "fig, ax = data_plot(scatter=[(X_new, y_new)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Fit a new perceptron classifier to the data using the same parameters as before.  It's very important that you keep the `shuffle=False` flag because we want the new blue point to be the last point encountered in the epoch.  Plot the new decision boundary.  How badly did the new rogue point disrupt the classifier? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_perc = Perceptron(max_iter=1, alpha=0.0, shuffle=False)\n",
    "new_perc.fit(X_new, y_new)\n",
    "xplot = np.linspace(-1.5, 1.5, 20)\n",
    "w, b = new_perc.coef_[0], new_perc.intercept_[0]\n",
    "yplot = # TODO \n",
    "fig, ax = data_plot(scatter=[(X_new, y_new)])\n",
    "ax.plot(xplot, yplot, lw=3, color=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part E**: Adjust some of the parameters and settings above to see if you can obtain a better decision boundary.  Things that might be helpful to change are the number of epochs and whether the data is shuffled between each epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br>\n",
    "<br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "### Helper Functions\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# Functions for KNN and iris dataset\n",
    "# ----------------------------------------------------\n",
    "\n",
    "def load_iris(standardize=False, random_state=1234): \n",
    "    \n",
    "    from sklearn import datasets\n",
    "    from sklearn.preprocessing import StandardScaler \n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Load the data and grab first two features \n",
    "    iris = datasets.load_iris()\n",
    "    X, y = iris.data[:,:2], iris.target \n",
    "        \n",
    "    # Randomly split into validation and training sets \n",
    "    ones = np.ones(50, dtype=int)\n",
    "    valid_mask = np.full(50, False)\n",
    "    valid_mask[np.random.choice(range(50), replace=False, size=16)] = True \n",
    "    train_mask = np.logical_not(valid_mask)\n",
    "    X_train = np.concatenate((X[y==0][train_mask], X[y==1][train_mask], X[y==2][train_mask]))\n",
    "    y_train = np.concatenate((0 * ones[train_mask], 1 * ones[train_mask], 2 * ones[train_mask]))\n",
    "    X_valid = np.concatenate((X[y==0][valid_mask], X[y==1][valid_mask], X[y==2][valid_mask]))\n",
    "    y_valid = np.concatenate((0 * ones[valid_mask], 1 * ones[valid_mask], 2 * ones[valid_mask]))\n",
    "    \n",
    "    # Standardize data if desired \n",
    "    if standardize: \n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_valid = scaler.transform(X_valid)\n",
    "        \n",
    "    return X_train, y_train, X_valid, y_valid, iris.target_names\n",
    "    \n",
    "\n",
    "def plot_iris(X, y):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,8))\n",
    "    name_color_dict = {\n",
    "        0: (\"steelblue\", \"setosa\"),\n",
    "        1:(\"#a76c6e\", \"versicolor\"),\n",
    "        2:(\"#6a9373\", \"virginica\")\n",
    "    }\n",
    "    for k in [0,1,2]:\n",
    "        ax.scatter(X[y==k, 0], X[y==k, 1], color=name_color_dict[k][0],\n",
    "                   s=100, label=name_color_dict[k][1])\n",
    "    ax.grid(alpha=0.25)\n",
    "    ax.legend(loc=\"upper right\", fontsize=16)\n",
    "    ax.set_xlabel(\"sepal length (cm)\", fontsize=16)\n",
    "    ax.set_ylabel(\"sepal width (cm)\", fontsize=16)\n",
    "    \n",
    "def plot_knn_boundary(X, y, model):\n",
    "    \n",
    "    from matplotlib import colors\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,8))\n",
    "    \n",
    "    name_color_dict = {\n",
    "        0: (\"steelblue\", \"setosa\"),\n",
    "        1:(\"#a76c6e\", \"versicolor\"),\n",
    "        2:(\"#6a9373\", \"virginica\")\n",
    "    }\n",
    "    for k in [0,1,2]:\n",
    "        ax.scatter(X[y==k, 0], X[y==k, 1], color=name_color_dict[k][0],\n",
    "                   s=100, label=name_color_dict[k][1], edgecolors=\"white\", zorder=2)\n",
    "    \n",
    "    # Plot the decision boundary. \n",
    "    x_min, x_max = X[:,0].min() - .5, X[:,0].max() + .5\n",
    "    y_min, y_max = X[:,1].min() - .5, X[:,1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.025), np.arange(y_min, y_max, 0.025))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    \n",
    "    # Define custom colormap \n",
    "    cmap = colors.ListedColormap(['steelblue', '#a76c6e', '#6a9373'])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.pcolormesh(xx, yy, Z, cmap=cmap, alpha=0.5, zorder=1)\n",
    "\n",
    "    ax.legend(loc=\"upper right\", fontsize=16)\n",
    "    ax.set_xlabel(\"sepal length (cm)\", fontsize=16)\n",
    "    ax.set_ylabel(\"sepal width (cm)\", fontsize=16)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Functions for Perceptron and Simulated Data \n",
    "# ----------------------------------------------------\n",
    "\n",
    "def data_plot(scatter=[], models=[]):\n",
    "    '''\n",
    "    Function to plot the dam data \n",
    "    '''\n",
    "    \n",
    "    # colors for scatter plots and model plots \n",
    "    scolors = [\"steelblue\", \"#a76c6e\", \"#6a9373\", \"orange\"]\n",
    "    mcolors = [\"black\", \"gray\"]\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6,6))\n",
    "    \n",
    "    # Loop over scatter data and make plots \n",
    "    for ii, (x, y) in enumerate(scatter):\n",
    "        pos = x[y==1, :]\n",
    "        neg = x[y==-1, :]\n",
    "        ax.scatter(pos[:, 0], pos[:, 1], s=100, color=scolors[1], label=\"pos\", zorder=2)\n",
    "        ax.scatter(neg[:, 0], neg[:, 1], s=100, color=scolors[0], label=\"neg\", zorder=2)\n",
    "        \n",
    "    # Loop over model data and make plots \n",
    "    for ii, (xplot, yplot, label) in enumerate(models):\n",
    "        ax.plot(xplot, yplot, color=mcolors[ii], lw=3, label=label, zorder=1)\n",
    "        \n",
    "    # Set axis limits\n",
    "    ax.set_xlim([-1.5,1.5])\n",
    "    ax.set_ylim([-1.5,1.5])\n",
    "        \n",
    "    # Label all the things \n",
    "    ax.set_xlabel(r\"$x_1$\", fontsize=16)\n",
    "    ax.set_ylabel(r\"$x_2$\", fontsize=16)\n",
    "    ax.set_title(\"Data Plot\", fontsize=20)\n",
    "    ax.grid(alpha=0.25)\n",
    "    ax.legend(loc=\"upper left\", fontsize=12)\n",
    "    \n",
    "    return fig, ax \n",
    "\n",
    "def poly_data(n, mag=0, sep=0, rot=0.0, random_state=1235):\n",
    "    np.random.seed(random_state)\n",
    "    x1 = np.random.uniform(-1, 1, n)\n",
    "    x2 = np.random.uniform(-1, 1, n)\n",
    "    X = np.column_stack((x1, x2))\n",
    "    y = np.array([1 if mag * (x1i-1) * (x1i+1) * x1i < x2i else -1\n",
    "                  for x1i, x2i in zip(x1, x2)])\n",
    "    X[y==1, 1] += sep \n",
    "    X[y==-1, 1] -= sep \n",
    "    s = np.sin(rot)\n",
    "    c = np.cos(rot)\n",
    "    Q = np.array([[c,s], [-s, c]])\n",
    "    X = np.dot(X, Q)\n",
    "    shuffle = np.random.choice(range(X.shape[0]), replace=False, size=X.shape[0])\n",
    "    X, y = X[shuffle, :], y[shuffle]\n",
    "    return X, y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
